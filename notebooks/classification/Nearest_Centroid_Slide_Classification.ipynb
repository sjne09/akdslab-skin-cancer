{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from typing import Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import histplot\n",
    "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
    "import torch\n",
    "\n",
    "from data_models.Label import NCLabel, Label\n",
    "from models.nearest_centroid.nearest_centroid import NearestCentroid\n",
    "from utils.load_data import SpecimenData\n",
    "from utils.slide_utils import plot_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization and utility methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load label-specimen mapping\n",
    "specimens_by_label = SpecimenData(\n",
    "    label_path=os.path.join(DATA_DIR, \"labels/labels.csv\")\n",
    ").specimens_by_label\n",
    "specimens_by_label = [set(spec_list) for spec_list in specimens_by_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tiles(embeds: dict, tiles_to_filter: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    filters out all tile embeddings that are not captured in the\n",
    "    tiles_to_filter list of coordinates\n",
    "    \"\"\"\n",
    "    coords = embeds[\"coords\"]\n",
    "    matches = (coords[:, None] == tiles_to_filter).all(-1)\n",
    "    mask = matches.any(dim=1)\n",
    "    return embeds[\"tile_embeds\"][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(\n",
    "    slides: list, models: list, tiles_to_filter: dict = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    get predictions for each slide across all models\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    for slide in slides:\n",
    "        # for each model, get predictions for each tile for a slide\n",
    "        slide_preds = []\n",
    "        for model in models:\n",
    "            with open(os.path.join(model[\"embedding_dir\"], slide), \"rb\") as f:\n",
    "                embeds = pickle.load(f)\n",
    "\n",
    "            embeds = (\n",
    "                extract_tiles(embeds, tiles_to_filter[slide[:-4]])\n",
    "                if tiles_to_filter\n",
    "                else embeds[\"tile_embeds\"]\n",
    "            )\n",
    "\n",
    "            slide_preds.append(\n",
    "                model[\"model\"]\n",
    "                .predict(embeds.float(), mode=\"dot_product\")\n",
    "                .softmax(dim=-1)\n",
    "            )  # (S, C)\n",
    "\n",
    "        # stack the prediction tensors into a single tensor;\n",
    "        # dim 0 is the model dimension\n",
    "        preds[slide[:-4]] = torch.stack(slide_preds)  # (M, S, C)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labeled_preds(preds: dict) -> dict:\n",
    "    \"\"\"\n",
    "    convert raw softmax preds to integer label preds;\n",
    "    eliminates the final dimension of the prediction tensors\n",
    "    \"\"\"\n",
    "    labeled_preds = {}\n",
    "    for slide, pred in preds.items():\n",
    "        labeled_preds[slide] = pred.argmax(dim=-1)  # (M, S)\n",
    "    return labeled_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_confusion_scores(labeled_preds: dict) -> Counter:\n",
    "    \"\"\"\n",
    "    get total model confusion counts;\n",
    "    1=all models agree on tile class,\n",
    "    3=all models disagree on tile\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for pred in labeled_preds.values():\n",
    "        pred: torch.Tensor\n",
    "        for tile in pred.transpose(1, 0):\n",
    "            counts[(len(tile.unique()))] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_by_gt(labeled_preds: dict):\n",
    "    \"\"\"\n",
    "    count confusion across all tiles within a certain classification group\n",
    "    \"\"\"\n",
    "    confusion_counters = {\"aggregate\": count_confusion_scores(labeled_preds)}\n",
    "    for label in Label:\n",
    "        idx = label.value\n",
    "        confusion_counters[label] = Counter()\n",
    "        for slide, pred in labeled_preds.items():\n",
    "            if slide[:6] in specimens_by_label[idx]:\n",
    "                pred: torch.Tensor\n",
    "                for tile in pred.transpose(1, 0):\n",
    "                    confusion_counters[label][(len(tile.unique()))] += 1\n",
    "        for i in range(3):\n",
    "            confusion_counters[label][i + 1] += 0\n",
    "    return confusion_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_slide_confusion(labeled_preds: dict) -> dict:\n",
    "    \"\"\"\n",
    "    confusion on a per-slide basis\n",
    "    \"\"\"\n",
    "    confusion = {}\n",
    "    for slide, pred in labeled_preds.items():\n",
    "        counter = Counter()\n",
    "        for tile in pred.transpose(1, 0):\n",
    "            counter[(len(tile.unique()))] += 1\n",
    "        confusion[slide] = counter\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_disagreement(confusion_by_slide: dict) -> list:\n",
    "    \"\"\"\n",
    "    isolate the counts of total disagreement\n",
    "    \"\"\"\n",
    "    disagreement_counts = []\n",
    "    for counter in confusion_by_slide.values():\n",
    "        tiles = 0\n",
    "        for count in counter.values():\n",
    "            tiles += count\n",
    "        disagreement_counts.append(counter.get(3, 0) / tiles)\n",
    "    return disagreement_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fms = [\"uni\", \"prism\", \"gigapath\"]\n",
    "experiments = [\n",
    "    \"old\",\n",
    "    \"new\",\n",
    "    \"sq_norm\",\n",
    "    \"top_pct\",\n",
    "    \"gaussian_mixture_separate\",\n",
    "    \"gaussian_mixture_combined\",\n",
    "]\n",
    "filtering_strategy = \"agg\"\n",
    "param_patterns = [\n",
    "    \"/opt/gpudata/skin-cancer/models/few-shot/intersects/{fm}_param2.pkl\",\n",
    "    \"/opt/gpudata/skin-cancer/models/few-shot/new/intersects/{fm}_param.pkl\",\n",
    "] + [\n",
    "    partial(\n",
    "        \"/opt/gpudata/skin-cancer/models/few-shot/new/filtered/{fm}_param-{method}-{filtering_strategy}.pkl\".format,\n",
    "        method=method,\n",
    "        filtering_strategy=filtering_strategy,\n",
    "    )\n",
    "    for method in experiments[2:]\n",
    "]\n",
    "embedding_dir = \"{root}/{fm}/tile_embeddings_sorted\"\n",
    "\n",
    "\n",
    "def load_model(param_path):\n",
    "    \"\"\"\n",
    "    loads a single model\n",
    "    \"\"\"\n",
    "    with open(param_path, \"rb\") as f:\n",
    "        param = pickle.load(f)\n",
    "        model = NearestCentroid(NCLabel, centroids=param, mode=\"intersects\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_models(param_path: str) -> list:\n",
    "    \"\"\"\n",
    "    load the models with associated metadata\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for fm in fms:\n",
    "        if isinstance(param_path, str):\n",
    "            model = load_model(param_path.format(fm=fm))\n",
    "        else:\n",
    "            model = load_model(\n",
    "                param_path(fm=fm)\n",
    "            )  # param path is an instance of partial\n",
    "        models.append(\n",
    "            {\n",
    "                \"fm\": fm,\n",
    "                \"embedding_dir\": embedding_dir.format(root=OUTPUT_DIR, fm=fm),\n",
    "                \"model\": model,\n",
    "            }\n",
    "        )\n",
    "    return models\n",
    "\n",
    "\n",
    "models = {\n",
    "    exp: load_models(param_patterns[i]) for i, exp in enumerate(experiments)\n",
    "}\n",
    "\n",
    "# get list of slides we have tile embeddings for\n",
    "all_slides = os.listdir(embedding_dir.format(root=OUTPUT_DIR, fm=fms[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get preds\n",
    "preds = {exp: get_preds(all_slides, models[exp]) for exp in experiments}\n",
    "labeled_preds = {exp: get_labeled_preds(preds[exp]) for exp in experiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_roi_tiles(roi_dir: str) -> dict:\n",
    "#     # get the roi tiles used for training the models\n",
    "#     tmp = NearestCentroid(NCLabel)\n",
    "#     tmp.fit(\n",
    "#         tile_embed_dir=os.path.join(OUTPUT_DIR, \"uni/tile_embeddings_sorted\"),\n",
    "#         roi_dir=roi_dir,\n",
    "#     )\n",
    "\n",
    "#     roi_tiles = tmp.roi_tiles\n",
    "#     del tmp\n",
    "#     return roi_tiles\n",
    "\n",
    "\n",
    "# roi_tiles_old = get_roi_tiles(\n",
    "#     \"/opt/gpudata/skin-cancer/models/few-shot/intersects\"\n",
    "# )\n",
    "# roi_tiles_new = get_roi_tiles(\n",
    "#     \"/opt/gpudata/skin-cancer/models/few-shot/new/intersects\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_roi_tiles_unlabeled(roi_tiles: dict):\n",
    "#     # separate the roi tiles from their lables and map slide\n",
    "#     # ids directly to tensors of coords (just unpacking the\n",
    "#     # dict returned by get_roi_tiles())\n",
    "#     roi_tiles_unlabeled = {}\n",
    "\n",
    "#     for label, slides in roi_tiles.items():\n",
    "#         for slide, tiles in slides.items():\n",
    "#             t = []\n",
    "#             for x in tiles:\n",
    "#                 # why did this happen?\n",
    "#                 if not isinstance(x, tuple):\n",
    "#                     print(label, x, slide)\n",
    "#                 else:\n",
    "#                     t.append(x)\n",
    "#             roi_tiles_unlabeled[slide] = torch.tensor(t, dtype=torch.float32)\n",
    "#     return roi_tiles_unlabeled\n",
    "\n",
    "\n",
    "# roi_tiles_unlabeled_old = get_roi_tiles_unlabeled(roi_tiles_old)\n",
    "# roi_tiles_unlabeled_new = get_roi_tiles_unlabeled(roi_tiles_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_roi_preds(roi_tiles_unlabeled, models):\n",
    "#     # get preds just for the tiles within the ROIs\n",
    "#     roi_preds = get_preds(\n",
    "#         [f\"{slide}.pkl\" for slide in roi_tiles_unlabeled.keys()],\n",
    "#         models,\n",
    "#         roi_tiles_unlabeled,\n",
    "#     )\n",
    "#     roi_labeled_preds = get_labeled_preds(roi_preds)\n",
    "#     return roi_labeled_preds\n",
    "\n",
    "\n",
    "# roi_labeled_preds_old = get_roi_preds(roi_tiles_unlabeled_old, old_models)\n",
    "# roi_labeled_preds_new = get_roi_preds(roi_tiles_unlabeled_new, new_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_odd_model_ratios(labeled_preds):\n",
    "    \"\"\"\n",
    "    get the ratio of the number of times a model was the\n",
    "    \"odd-one-out\" on a prediction (i.e., when confusion=2)\n",
    "    \"\"\"\n",
    "    odd_model = []\n",
    "    for preds in labeled_preds.values():\n",
    "        for col in preds.T:\n",
    "            if len(col.unique()) == 2:\n",
    "                if col[0] == col[1]:\n",
    "                    odd_model.append(2)\n",
    "                elif col[1] == col[2]:\n",
    "                    odd_model.append(0)\n",
    "                else:\n",
    "                    odd_model.append(1)\n",
    "    _, odd_model_counts = np.unique(np.array(odd_model), return_counts=True)\n",
    "    odd_model_ratios = odd_model_counts / odd_model_counts.sum()\n",
    "    return odd_model_ratios\n",
    "\n",
    "\n",
    "odd_models = {\n",
    "    exp: {\n",
    "        fms[i]: x\n",
    "        for i, x in enumerate(get_odd_model_ratios(labeled_preds[exp]))\n",
    "    }\n",
    "    for exp in experiments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in experiments:\n",
    "    print(f\"{exp}: {odd_models[exp]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate confusion scores between models for each tile assessed. Confusion scores are:  \n",
    "1 = all models agree  \n",
    "2 = one model disagrees  \n",
    "3 = all models disagree  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_counts = {}\n",
    "for exp in experiments:\n",
    "    confusion_counts[exp] = count_confusion_scores(labeled_preds[exp])\n",
    "    print(f\"{exp}: {confusion_counts[exp]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of disagreement across all slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: proportion of tiles in disagreement\n",
    "# y: proportion of slides with a given level of disagreement\n",
    "for exp in experiments:\n",
    "    confusion = per_slide_confusion(labeled_preds[exp])\n",
    "    disagreement_counts = count_disagreement(confusion)\n",
    "    # visualize the distribution of total disagreement\n",
    "    histplot(\n",
    "        disagreement_counts,\n",
    "        element=\"step\",\n",
    "        fill=False,\n",
    "        stat=\"proportion\",\n",
    "        label=exp,\n",
    "    )\n",
    "plt.xlim(left=0, right=0.2)\n",
    "plt.ylim(bottom=0, top=0.25)\n",
    "plt.xlabel(\"Proportion of tiles in total disagreement\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, roi_labeled_preds in enumerate(\n",
    "#     [roi_labeled_preds_old, roi_labeled_preds_new]\n",
    "# ):\n",
    "#     confusion = per_slide_confusion(roi_labeled_preds)\n",
    "#     disagreement_counts = count_disagreement(confusion)\n",
    "#     # visualize the distribution of total disagreement\n",
    "#     histplot(\n",
    "#         disagreement_counts,\n",
    "#         element=\"step\",\n",
    "#         fill=True,\n",
    "#         stat=\"proportion\",\n",
    "#         label=model_version[i],\n",
    "#     )\n",
    "# plt.xlim(left=0)\n",
    "# plt.xlabel(\"Proportion of tiles in disagreement\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize confusion levels across all slides vs. across ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_counters = [\n",
    "#     [\n",
    "#         confusion_by_gt(all_labeled_preds_old),\n",
    "#         confusion_by_gt(all_labeled_preds_new),\n",
    "#         confusion_by_gt(all_labeled_preds_filt),\n",
    "#     ],\n",
    "#     [\n",
    "#         confusion_by_gt(roi_labeled_preds_old),\n",
    "#         confusion_by_gt(roi_labeled_preds_new),\n",
    "#     ],\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_counters = {\n",
    "    exp: confusion_by_gt(labeled_preds[exp]) for exp in experiments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion levels\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12), sharey=True)\n",
    "for i, axr in enumerate(axs):\n",
    "    for j, ax in enumerate(axr):\n",
    "        exp = experiments[i * 3 + j]\n",
    "        conf_counter = confusion_counters[exp]\n",
    "        df = pd.DataFrame(conf_counter).T\n",
    "        df[list(range(1, 4))].div(df.sum(axis=1), axis=0).plot(\n",
    "            kind=\"bar\", ax=ax, legend=False\n",
    "        )\n",
    "        ax.set_xticks(\n",
    "            ticks=list(range(df.shape[0])),\n",
    "            labels=[\n",
    "                tick.name if isinstance(tick, Label) else tick\n",
    "                for tick in df.index\n",
    "            ],\n",
    "        )\n",
    "        ax.set_xlabel(\"specimen label\")\n",
    "        ax.set_ylabel(\"% of tiles\")\n",
    "        ax.set_title(exp)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of tiles that are predicted (obviously) incorrectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels that are obviously incorrect for a given gt label\n",
    "obv_incorrect_labels = {\n",
    "    Label.na: {\n",
    "        NCLabel.bcc_nodular,\n",
    "        NCLabel.bcc_superficial,\n",
    "        NCLabel.bowens,\n",
    "        NCLabel.scc,\n",
    "    },\n",
    "    Label.bcc: {NCLabel.bowens, NCLabel.scc},\n",
    "    Label.bowens: {NCLabel.bcc_nodular, NCLabel.bcc_superficial},\n",
    "    Label.scc: {NCLabel.bcc_nodular, NCLabel.bcc_superficial},\n",
    "}\n",
    "\n",
    "# convert the values from above to tensors for use in torch.isin\n",
    "for gt in obv_incorrect_labels:\n",
    "    obv_incorrect_labels[gt] = torch.tensor(\n",
    "        [x.value for x in obv_incorrect_labels[gt]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vote_preds(all_labeled_preds) -> dict:\n",
    "    \"\"\"\n",
    "    compute the mode prediction for each tile across the models\"\n",
    "    \"\"\"\n",
    "    vote_preds = {}\n",
    "    for slide, pred in all_labeled_preds.items():\n",
    "        vote_preds[slide] = pred.mode(dim=0).values\n",
    "    return vote_preds\n",
    "\n",
    "\n",
    "vote_preds = {exp: get_vote_preds(labeled_preds[exp]) for exp in experiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_incorrect(preds: Dict[str, torch.Tensor]) -> Dict[Label, Counter]:\n",
    "    \"\"\"\n",
    "    get counts of tiles obviously incorrectly predicted\n",
    "    \"\"\"\n",
    "    obv_incorrect_counts = {}\n",
    "    for label in Label:\n",
    "        idx = label.value\n",
    "        obv_incorrect_counts[label] = Counter()\n",
    "        for slide, pred in preds.items():\n",
    "            # if ground truth matches label, then count obv incorrect\n",
    "            if slide[:6] in specimens_by_label[idx]:\n",
    "                mask = torch.isin(pred, obv_incorrect_labels[label])\n",
    "                for tile_pred in pred[mask]:\n",
    "                    obv_incorrect_counts[label][tile_pred.item()] += 1\n",
    "\n",
    "    return obv_incorrect_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_counts(\n",
    "    all_labeled_preds: dict, vote_preds: dict = None\n",
    ") -> dict:\n",
    "    incorrect_counts = {\n",
    "        fm: count_incorrect(\n",
    "            {slide: pred[i] for slide, pred in all_labeled_preds.items()}\n",
    "        )\n",
    "        for i, fm in enumerate(fms)\n",
    "    }\n",
    "    if vote_preds:\n",
    "        incorrect_counts[\"vote\"] = count_incorrect(vote_preds)\n",
    "    return incorrect_counts\n",
    "\n",
    "\n",
    "incorrect_counts = {\n",
    "    exp: get_incorrect_counts(labeled_preds[exp], vote_preds[exp])\n",
    "    for exp in experiments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_counts[\"gaussian_mixture_combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of obviously incorrect classifications for benign slides\n",
    "# by classification\n",
    "df = pd.DataFrame(confusion_counters[experiments[0]]).T\n",
    "for exp in experiments:\n",
    "    print(f\"**{exp}**\")\n",
    "    for model, counts in incorrect_counts[exp].items():\n",
    "        print(model)\n",
    "        for incorrect, count in counts[Label.na].items():\n",
    "            # numerator: num incorrect; denominator: total benign gt slides\n",
    "            print(\n",
    "                f\"{NCLabel(incorrect).name}: {count / df.loc[Label.na].sum()}\"\n",
    "            )\n",
    "        print()\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_proportions(ground_truth: Label, preds: dict):\n",
    "    # preds must be labeled, ie {slide_id: tensor with shape (# models, # tiles)}\n",
    "    # and values in tensors are integers\n",
    "    gt_label = ground_truth.value\n",
    "    props = np.zeros((len(all_slides), len(NCLabel._member_names_)))\n",
    "    slides = []\n",
    "\n",
    "    i = 0\n",
    "    for slide_id in all_slides:\n",
    "        if slide_id[:6] in specimens_by_label[gt_label]:\n",
    "            slides.append(slide_id[:-4])\n",
    "            counts = preds[slide_id[:-4]].unique(return_counts=True)\n",
    "            tile_count = counts[1].sum()\n",
    "            for j, label in enumerate(counts[0]):\n",
    "                props[i][label] = counts[1][j] / tile_count\n",
    "            i += 1\n",
    "    props = props[:i].T\n",
    "    return props, slides\n",
    "\n",
    "\n",
    "def proportions_hist(\n",
    "    class_props: list, ax, ground_truth: Label, experiment_name: str = \"\"\n",
    "):\n",
    "    bins = np.linspace(0, 1, 100)\n",
    "\n",
    "    for i, pcts in enumerate(class_props):\n",
    "        if i not in {6}:\n",
    "            histplot(\n",
    "                np.array(pcts),\n",
    "                bins=bins,\n",
    "                element=\"step\",\n",
    "                fill=False,\n",
    "                stat=\"proportion\",\n",
    "                label=NCLabel(i).name,\n",
    "                ax=ax,\n",
    "            )\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"{ground_truth.name}-{exp}\")\n",
    "        ax.set_xlabel(\"proportion of tiles per slide\")\n",
    "        ax.set_ylabel(\"proportion of slides\")\n",
    "        ax.set_xlim(left=0, right=0.15)\n",
    "\n",
    "\n",
    "def plot_heatmap(\n",
    "    slide_id: str,\n",
    "    preds: torch.Tensor,\n",
    "    out_name: str,\n",
    "    title: str,\n",
    "    embedding_dir: str,\n",
    "):\n",
    "    if os.path.exists(out_name):\n",
    "        return\n",
    "\n",
    "    with open(os.path.join(embedding_dir, f\"{slide_id}.pkl\"), \"rb\") as f:\n",
    "        slide_data = pickle.load(f)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), constrained_layout=True)\n",
    "    plot_image(\n",
    "        fpath=\"/opt/gpudata/skin-cancer/data/slides/\" + f\"{slide_id}.svs\",\n",
    "        ax=ax,\n",
    "        tile_coords=slide_data[\"coords\"],\n",
    "        tile_weights=preds,\n",
    "        weight_labels={label.name: label.value for label in NCLabel},\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    fig.savefig(\n",
    "        out_name,\n",
    "        bbox_inches=\"tight\",\n",
    "        dpi=200,\n",
    "    )\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all slides, get percent of each class\n",
    "props = {\n",
    "    exp: {label: class_proportions(label, vote_preds[exp]) for label in Label}\n",
    "    for exp in experiments\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of proportion of tiles assigned to each class\n",
    "# per slide\n",
    "label_of_interest = Label.bcc\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 12), sharey=True, sharex=True)\n",
    "for k, exp in enumerate(experiments):\n",
    "    i = k // 3\n",
    "    j = k % 3\n",
    "    proportions_hist(\n",
    "        props[exp][label_of_interest][0], axs[i][j], label_of_interest, exp\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_specs = {\n",
    "    \"bowens\": \"660524-2\",\n",
    "    \"bcc\": \"660369-6\",\n",
    "    \"scc\": \"660109-1\",\n",
    "    \"na\": \"660375-1\",\n",
    "}\n",
    "embedding_dir = models[experiments[0]][0][\"embedding_dir\"]\n",
    "\n",
    "for exp in experiments:\n",
    "    model_outputs = {\n",
    "        fm: f\"labeled_preds[exp].get(slide_id)[{i}]\"\n",
    "        for i, fm in enumerate(fms)\n",
    "    }\n",
    "    model_outputs[\"vote\"] = \"vote_preds[exp].get(slide_id)\"\n",
    "\n",
    "    for i, (fm, pred_string) in enumerate(model_outputs.items()):\n",
    "        for ground_truth, slide_id in sampled_specs.items():\n",
    "            print(f\"{exp}-{fm}-{slide_id}\")\n",
    "            preds = eval(pred_string)\n",
    "            plot_heatmap(\n",
    "                slide_id,\n",
    "                preds,\n",
    "                f\"{exp}-{fm}-{slide_id}.png\",\n",
    "                f\"{exp}-{fm}-{slide_id}\",\n",
    "                embedding_dir,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.zeros((len(all_slides), len(Label._member_names_)))\n",
    "for i, slide_id in enumerate(all_slides):\n",
    "    for j, lst in enumerate(specimens_by_label):\n",
    "        if slide_id[:6] in lst:\n",
    "            onehot[i][j] = 1\n",
    "\n",
    "props = np.zeros((len(all_slides), len(NCLabel._member_names_)))\n",
    "for i, slide_id in enumerate(all_slides):\n",
    "    counts = vote_preds[\"gaussian_mixture_separate\"][slide_id[:-4]].unique(\n",
    "        return_counts=True\n",
    "    )\n",
    "    tile_count = counts[1].sum()\n",
    "    for j, label in enumerate(counts[0]):\n",
    "        props[i][label] = counts[1][j] / tile_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "plot = RocCurveDisplay.from_predictions\n",
    "plot(onehot[:, 0], props[:, :2].sum(axis=-1), name=\"benign\", ax=ax)\n",
    "plot(onehot[:, 1], props[:, 2], name=\"bowens\", ax=ax)\n",
    "plot(onehot[:, 2], props[:, 3:5].sum(axis=-1), name=\"bcc\", ax=ax)\n",
    "plot(onehot[:, 3], props[:, 5], name=\"scc\", ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gigapath_clone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
