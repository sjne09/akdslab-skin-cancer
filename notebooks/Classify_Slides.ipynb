{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_data import load_data\n",
    "\n",
    "label_path = os.path.join(DATA_DIR, \"labels/labels.csv\")\n",
    "embedding_path = os.path.join(OUTPUT_DIR, \"uni/uni_pooled_slide_embeds.pkl\")\n",
    "fold_path = os.path.join(DATA_DIR, \"folds.json\")\n",
    "\n",
    "df = load_data(\n",
    "    label_path=label_path, embedding_path=embedding_path, fold_path=fold_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map specimen id to a list of WSIs\n",
    "specs = df.groupby(\"specimen_id\").groups\n",
    "specs = {k: list(v) for k, v in specs.items()}\n",
    "\n",
    "# get list of slides within each fold\n",
    "slide_folds = df.groupby(\"fold\").groups\n",
    "slide_folds = [list(slides) for slides in slide_folds.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_models.Label import Label\n",
    "\n",
    "specimen_df = df.reset_index()[\n",
    "    [\"specimen_id\", \"bowens\", \"scc\", \"bcc\", \"na\"]\n",
    "].drop_duplicates(subset=[\"specimen_id\"])\n",
    "\n",
    "spec_freqs = {\n",
    "    label: specimen_df[label].value_counts(normalize=True).iloc[1]\n",
    "    for label in Label._member_names_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"embedding\"]\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_val_split(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    val_fold_indices: List[str],\n",
    "    z_norm: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Construct training and validation sets from the provided data using\n",
    "    the given indices for the validation set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        The feature set\n",
    "\n",
    "    y : pd.DataFrame\n",
    "        The targets\n",
    "\n",
    "    val_fold_indices : List[str]\n",
    "        The indices of samples to include in the validation set\n",
    "\n",
    "    z_norm : bool\n",
    "        Whether to use z-score normalization / standardization\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "        X_train, y_train, X_val, y_val\n",
    "    \"\"\"\n",
    "    X_split = {}\n",
    "    y_split = {}\n",
    "    X_split[\"val\"] = X.loc[val_fold_indices]\n",
    "    y_split[\"val\"] = y.loc[val_fold_indices]\n",
    "    X_split[\"train\"] = X[X.index.difference(X_split[\"val\"].index)]\n",
    "    y_split[\"train\"] = y[y.index.difference(y_split[\"val\"].index)]\n",
    "    assert X_split[\"val\"].index == y_split[\"val\"].index\n",
    "    assert X_split[\"train\"].index == y_split[\"train\"].index\n",
    "\n",
    "    for split, embs in X_split.items():\n",
    "        embs = np.stack(embs.to_list())\n",
    "        if z_norm:\n",
    "            embs = (embs - embs.mean(axis=1, keepdims=True)) / embs.std(\n",
    "                axis=1, keepdims=True\n",
    "            )\n",
    "\n",
    "        X_split[split] = embs\n",
    "        y_split[split] = y_split[split].to_numpy()\n",
    "\n",
    "    return (\n",
    "        X_split[\"train\"],\n",
    "        y_split[\"train\"],\n",
    "        X_split[\"val\"],\n",
    "        y_split[\"val\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_level_probs(\n",
    "    slide_indices: List[str], probs: np.ndarray\n",
    ") -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculates the average model output probabilities for each class\n",
    "    at the specimen level rather than slide level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    slide_indices : List[str]\n",
    "        A list of slide ids for which the specimen is the first 6 chars\n",
    "\n",
    "    probs : np.ndarray\n",
    "        Classifier output probabilities for each slide included in\n",
    "        slide_indices; shape (N, O) where N is the number of slides\n",
    "        and O is the number of classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        The specimen ids\n",
    "\n",
    "    np.ndarray\n",
    "        The average probabilities for each specimen; shape (M, O) where\n",
    "        M is the number of specimens and O is the number of classes\n",
    "    \"\"\"\n",
    "    combined_probs = {}\n",
    "    for j, slide_idx in enumerate(slide_indices):\n",
    "        spec = slide_idx[:6]\n",
    "        if combined_probs.get(spec) is None:\n",
    "            combined_probs[spec] = [probs[j]]\n",
    "        else:\n",
    "            combined_probs[spec].append(probs[j])\n",
    "\n",
    "    for k in combined_probs.keys():\n",
    "        combined_probs[k] = np.array(combined_probs[k])\n",
    "        combined_probs[k] = (\n",
    "            combined_probs[k].sum(axis=0) / combined_probs[k].shape[0]\n",
    "        )\n",
    "\n",
    "    return list(combined_probs.keys()), np.array(list(combined_probs.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import PrecisionRecallDisplay, RocCurveDisplay, auc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_eval(\n",
    "    mean_x: np.ndarray,\n",
    "    onehot_labels: np.ndarray,\n",
    "    probs: np.ndarray,\n",
    "    ax: plt.Axes,\n",
    "    type: str,\n",
    "    fold_idx: int,\n",
    "    plot_chance_level: bool,\n",
    "):\n",
    "    # set eval type specific vars\n",
    "    if type == \"ROC\":\n",
    "        curve_display = RocCurveDisplay\n",
    "        set_initial_interp_value = True\n",
    "        interp_transform = lambda x: x\n",
    "        x_attr = \"fpr\"\n",
    "        y_attr = \"tpr\"\n",
    "        auc_attr = \"roc_auc\"\n",
    "    else:\n",
    "        curve_display = PrecisionRecallDisplay\n",
    "        set_initial_interp_value = False\n",
    "        interp_transform = lambda x: np.flip(x)\n",
    "        x_attr = \"recall\"\n",
    "        y_attr = \"precision\"\n",
    "        auc_attr = \"average_precision\"\n",
    "\n",
    "    # create the curve\n",
    "    curve = curve_display.from_predictions(\n",
    "        onehot_labels,\n",
    "        probs,\n",
    "        name=f\"{type} fold {fold_idx}\",\n",
    "        plot_chance_level=plot_chance_level,\n",
    "        ax=ax,\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "    )\n",
    "\n",
    "    # interpolate results using mean_x param to allow for averaging of curves\n",
    "    interp_y = np.interp(\n",
    "        mean_x,\n",
    "        interp_transform(getattr(curve, x_attr)),\n",
    "        interp_transform(getattr(curve, y_attr)),\n",
    "    )\n",
    "    interp_y[0] = 0.0 if set_initial_interp_value else interp_y[0]\n",
    "\n",
    "    return interp_y, getattr(curve, auc_attr)\n",
    "\n",
    "\n",
    "def create_mean_curve(\n",
    "    mean_x: np.ndarray,\n",
    "    cv_y: np.ndarray,\n",
    "    aucs: np.ndarray,\n",
    "    ax: plt.Axes,\n",
    "    type: str,\n",
    "    class_name: str,\n",
    "):\n",
    "    if type == \"ROC\":\n",
    "        set_final_mean_value = True\n",
    "        ax.set(\n",
    "            xlabel=\"False Positive Rate\",\n",
    "            ylabel=\"True Positive Rate\",\n",
    "            title=f\"Mean ROC curve with variability for {class_name}\",\n",
    "        )\n",
    "    else:\n",
    "        set_final_mean_value = False\n",
    "        ax.set(\n",
    "            xlabel=\"Recall\",\n",
    "            ylabel=\"Precision\",\n",
    "            title=f\"Mean precision-recall curve with variability for {class_name}\",\n",
    "        )\n",
    "\n",
    "    # prep plot data\n",
    "    mean_y = np.mean(cv_y, axis=0)\n",
    "    mean_y[-1] = 1.0 if set_final_mean_value else mean_y[-1]\n",
    "    mean_auc = auc(mean_x, mean_y)\n",
    "    std_auc = np.std(aucs)\n",
    "    auc_label = \"AUC\" if type == \"ROC\" else \"AP\"\n",
    "\n",
    "    # plot the mean line\n",
    "    ax.plot(\n",
    "        mean_x,\n",
    "        mean_y,\n",
    "        color=\"b\",\n",
    "        label=r\"Mean %s (%s = %0.2f $\\pm$ %0.2f)\"\n",
    "        % (type, auc_label, mean_auc, std_auc),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "\n",
    "    # plot the standard deviation bands\n",
    "    std_y = np.std(cv_y, axis=0)\n",
    "    y_upper = np.minimum(mean_y + std_y, 1)\n",
    "    y_lower = np.maximum(mean_y - std_y, 0)\n",
    "    ax.fill_between(\n",
    "        mean_x,\n",
    "        y_lower,\n",
    "        y_upper,\n",
    "        color=\"grey\",\n",
    "        alpha=0.2,\n",
    "        label=r\"$\\pm$ 1 std. dev.\",\n",
    "    )\n",
    "\n",
    "    ax.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(clf, folds: list, y_onehot: pd.DataFrame, exp_name: str):\n",
    "    # dictionaries to keep eval curve results for each label\n",
    "    tprs = {label: [] for label in Label._member_names_}\n",
    "    aucs = {label: [] for label in Label._member_names_}\n",
    "    precisions = {label: [] for label in Label._member_names_}\n",
    "    aps = {label: [] for label in Label._member_names_}\n",
    "\n",
    "    # mean x vals and axes for curves\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_recall = np.linspace(0, 1, 100)\n",
    "    roc_fig, roc_axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    prc_fig, prc_axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    for i, val_fold_indices in enumerate(folds):\n",
    "        # fit the classifier on the train data and extract probs\n",
    "        X_train, y_train, X_val, _ = train_val_split(\n",
    "            X, y, val_fold_indices, True\n",
    "        )\n",
    "        clf.fit(X_train, y_train)\n",
    "        probs = clf.predict_proba(X_val)\n",
    "\n",
    "        # probs are on slide-level - need specimen level\n",
    "        _, probs = get_spec_level_probs(val_fold_indices, probs)\n",
    "\n",
    "        # get onehot labels for val set\n",
    "        y_onehot_val = y_onehot.loc[val_fold_indices].drop_duplicates(\n",
    "            subset=[\"specimen_id\"]\n",
    "        )\n",
    "\n",
    "        # for each predicted class, plot the current classifier's eval\n",
    "        # and retain relevant data in dicts\n",
    "        for j, class_of_interest in enumerate(Label._member_names_):\n",
    "            interp_tpr, roc_auc = plot_eval(\n",
    "                mean_x=mean_fpr,\n",
    "                onehot_labels=y_onehot_val[f\"{class_of_interest}\"],\n",
    "                probs=probs[:, Label[class_of_interest].value],\n",
    "                ax=roc_axs[j // 2][j % 2],\n",
    "                type=\"ROC\",\n",
    "                fold_idx=i,\n",
    "                plot_chance_level=i == len(folds) - 1,\n",
    "            )\n",
    "            tprs[class_of_interest].append(interp_tpr)\n",
    "            aucs[class_of_interest].append(roc_auc)\n",
    "\n",
    "            interp_precision, average_precision = plot_eval(\n",
    "                mean_x=mean_recall,\n",
    "                onehot_labels=y_onehot_val[f\"{class_of_interest}\"],\n",
    "                probs=probs[:, Label[class_of_interest].value],\n",
    "                ax=prc_axs[j // 2][j % 2],\n",
    "                type=\"PRC\",\n",
    "                fold_idx=i,\n",
    "                plot_chance_level=False,\n",
    "            )\n",
    "            precisions[class_of_interest].append(interp_precision)\n",
    "            aps[class_of_interest].append(average_precision)\n",
    "\n",
    "    # plot the mean eval curves\n",
    "    for j, class_of_interest in enumerate(Label._member_names_):\n",
    "        create_mean_curve(\n",
    "            mean_fpr,\n",
    "            tprs[class_of_interest],\n",
    "            aucs[class_of_interest],\n",
    "            roc_axs[j // 2][j % 2],\n",
    "            \"ROC\",\n",
    "            class_of_interest,\n",
    "        )\n",
    "\n",
    "        prc_ax = prc_axs[j // 2][j % 2]\n",
    "        create_mean_curve(\n",
    "            mean_recall,\n",
    "            precisions[class_of_interest],\n",
    "            aps[class_of_interest],\n",
    "            prc_ax,\n",
    "            \"PRC\",\n",
    "            class_of_interest,\n",
    "        )\n",
    "\n",
    "        # add chance line for PRC == label freq at specimen level\n",
    "        prc_ax.axhline(\n",
    "            spec_freqs[class_of_interest],\n",
    "            linestyle=\"--\",\n",
    "            label=r\"Chance level (AP = %0.2f)\"\n",
    "            % (spec_freqs[class_of_interest]),\n",
    "            color=\"black\",\n",
    "        )\n",
    "\n",
    "        # reorder legend\n",
    "        handles, labs = prc_ax.get_legend_handles_labels()\n",
    "        handles[-1], handles[-3] = handles[-3], handles[-1]\n",
    "        labs[-1], labs[-3] = labs[-3], labs[-1]\n",
    "        prc_ax.legend(handles=handles, labels=labs, loc=\"lower right\")\n",
    "\n",
    "    roc_fig.show()\n",
    "    roc_fig.savefig(f\"outputs/{exp_name}-roc.png\")\n",
    "    prc_fig.show()\n",
    "    prc_fig.savefig(f\"outputs/{exp_name}-prc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = df[[\"bowens\", \"scc\", \"bcc\", \"na\", \"specimen_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(objective=\"multi:softmax\", num_class=4)\n",
    "crossval(clf, slide_folds, y_onehot, \"uni/global_pooling/uni-xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, solver=\"saga\")\n",
    "crossval(clf, slide_folds, y_onehot, \"uni/global_pooling/uni-lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log_loss\")\n",
    "\n",
    "crossval(clf, slide_folds, y_onehot, \"uni/global_pooling/uni-sgd_lr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
