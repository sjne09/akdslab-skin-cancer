{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels data with folds\n",
    "import numpy as np\n",
    "\n",
    "from utils.load_data import load_data\n",
    "from data_models.Label import Label\n",
    "\n",
    "label_path = os.path.join(DATA_DIR, \"labels/labels.csv\")\n",
    "fold_path = os.path.join(DATA_DIR, \"folds.json\")\n",
    "\n",
    "df = load_data(label_path=label_path, fold_path=fold_path)\n",
    "df = df.set_index(\"specimen_id\")\n",
    "labels_onehot = df[Label._member_names_].to_dict(orient=\"split\", index=True)\n",
    "labels_onehot = {\n",
    "    k: np.array(labels_onehot[\"data\"][i])\n",
    "    for i, k in enumerate(labels_onehot[\"index\"])\n",
    "}\n",
    "labels_dict = {row.name: int(row[\"label\"]) for _, row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundation_model = \"uni\"\n",
    "\n",
    "# get the absolute path for each slide's set of tile embeddings\n",
    "tile_embed_dir = f\"/opt/gpudata/skin-cancer/outputs/{foundation_model}/tile_embeddings_sorted\"\n",
    "fnames = os.listdir(tile_embed_dir)\n",
    "tile_embed_paths = [\n",
    "    os.path.join(tile_embed_dir, fname)\n",
    "    for fname in fnames\n",
    "    if fname.endswith(\".pkl\") and fname[:6] in set(df.index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of specimens within each fold\n",
    "specimens_by_fold = df.groupby(\"fold\").groups\n",
    "specimens_by_fold = [list(specs) for specs in specimens_by_fold.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map specimens to slides\n",
    "slides_by_specimen = {spec: [] for spec in list(df.index)}\n",
    "for slide in tile_embed_paths:\n",
    "    slide_name = os.path.basename(slide)[:-4]\n",
    "    spec = slide_name[:6]\n",
    "    if slides_by_specimen.get(spec) is not None:\n",
    "        slides_by_specimen[spec].append(slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_models.Label import Label\n",
    "\n",
    "class_freqs = {\n",
    "    label: df[label].value_counts(normalize=True).iloc[1]\n",
    "    for label in Label._member_names_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_models.datasets import SlideEncodingDataset, collate_tile_embeds\n",
    "from utils.split import train_val_split_slides, train_val_split_labels\n",
    "\n",
    "\n",
    "def get_loaders(\n",
    "    val_fold: int,\n",
    "    specimens_by_fold: List[List[str]],\n",
    "    slides_by_specimen: Dict[str, List[str]],\n",
    "    labels_by_specimen: Dict[str, int],\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train, val = train_val_split_slides(\n",
    "        val_fold=val_fold,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "        slides_by_specimen=slides_by_specimen,\n",
    "    )\n",
    "    train_labels, val_labels = train_val_split_labels(\n",
    "        val_fold=val_fold,\n",
    "        labels_by_specimen=labels_by_specimen,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        SlideEncodingDataset(train, train_labels),\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_tile_embeds,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        SlideEncodingDataset(val, val_labels),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_tile_embeds,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from models.agg import MILClassifier\n",
    "from models.utils.train import train_epoch, val_epoch\n",
    "from evaluation.eval import Evaluator\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 4\n",
    "PATIENCE = 10\n",
    "\n",
    "gates = [False]\n",
    "head_counts = [1]\n",
    "auroc_keys = [k + \"_auroc\" for k in [\"benign\", \"bowens\", \"bcc\", \"scc\"]]\n",
    "auprc_keys = [k + \"_auprc\" for k in [\"benign\", \"bowens\", \"bcc\", \"scc\"]]\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"foundation_model\", \"aggregator\", \"classifier\", \"fold\"]\n",
    "    + auroc_keys\n",
    "    + auprc_keys\n",
    ")\n",
    "\n",
    "for exp_idx in range(len(gates)):\n",
    "    print(f\"\\n\\n****EXPERIMENT {exp_idx + 1}****\\n\\n\")\n",
    "    # set experiment-specific variables\n",
    "    gated = gates[exp_idx]\n",
    "    heads = head_counts[exp_idx]\n",
    "    aggregator = \"gabmil_test\" if gated else \"abmil_test\"\n",
    "    save_directory = \"gated\" if gated else \"ungated\"\n",
    "    model_name = (\n",
    "        \"chkpts/{foundation_model}-{aggregator}-{heads}_heads-fold-{i}.pt\"\n",
    "    )\n",
    "    # exp_name = f\"{foundation_model}/abmil/{save_directory}/{foundation_model}-3_hidden-{heads}_heads\"\n",
    "    exp_name = (\n",
    "        f\"{foundation_model}/testing/{foundation_model}-3_hidden-{heads}_heads\"\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(Label)\n",
    "\n",
    "    # for each fold, train and validate while saving the best models\n",
    "    # then evaluate the final outputs by generating curves using Evaluator\n",
    "    for i in range(len(specimens_by_fold)):\n",
    "        print(f\"--------------------FOLD    {i + 1}--------------------\")\n",
    "        # get dataloaders and embed dims for loaded embeddings\n",
    "        train_loader, val_loader = get_loaders(\n",
    "            i, specimens_by_fold, slides_by_specimen, labels_dict\n",
    "        )\n",
    "        for sample in train_loader:\n",
    "            embed_dim = sample[\"tile_embeds\"].shape[-1]\n",
    "            break\n",
    "\n",
    "        model = MILClassifier(embed_dim, NUM_LABELS, heads, gated).to(device)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "        best_loss = float(\"inf\")\n",
    "        best_model_weights = None\n",
    "        best_model_data = {\"ids\": None, \"labels\": None, \"probs\": None}\n",
    "        patience = PATIENCE\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss = train_epoch(\n",
    "                model,\n",
    "                train_loader,\n",
    "                optim,\n",
    "                loss_fn,\n",
    "                BATCH_SIZE,\n",
    "                [\"tile_embeds\", \"pos\"],\n",
    "                \"label\",\n",
    "                device,\n",
    "            )\n",
    "            val_loss, labels, probs, ids = val_epoch(\n",
    "                model,\n",
    "                val_loader,\n",
    "                device,\n",
    "                [\"tile_embeds\", \"pos\"],\n",
    "                \"label\",\n",
    "                loss_fn,\n",
    "            )\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                best_model_data[\"ids\"] = ids\n",
    "                best_model_data[\"labels\"] = labels\n",
    "                best_model_data[\"probs\"] = probs\n",
    "                patience = PATIENCE\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience == 0:\n",
    "                    break\n",
    "\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                spaces = \" \" * (4 - len(str(epoch + 1)))\n",
    "                print(\n",
    "                    f\"--------------------EPOCH{spaces}{epoch + 1}--------------------\"\n",
    "                )\n",
    "                print(f\"train loss: {train_loss:0.6f}\")\n",
    "                print(f\"val loss:   {val_loss:0.6f}\")\n",
    "                print()\n",
    "\n",
    "        # save the best model\n",
    "        torch.save(\n",
    "            best_model_weights,\n",
    "            os.path.join(\n",
    "                OUTPUT_DIR,\n",
    "                model_name.format(\n",
    "                    foundation_model=foundation_model,\n",
    "                    aggregator=aggregator,\n",
    "                    heads=heads,\n",
    "                    i=i,\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # extract relevant data from val results for best model\n",
    "        ids, probs = Evaluator.get_spec_level_probs(\n",
    "            best_model_data[\"ids\"], best_model_data[\"probs\"]\n",
    "        )\n",
    "        labels_onehot_val = np.array(itemgetter(*ids)(labels_onehot))\n",
    "\n",
    "        evaluator.fold(probs, labels_onehot_val, i, len(specimens_by_fold))\n",
    "        auroc = roc_auc_score(\n",
    "            labels_onehot_val, probs, average=None, multi_class=\"ovr\"\n",
    "        )\n",
    "        auroc_dict = {auroc_keys[i]: v for i, v in enumerate(auroc)}\n",
    "\n",
    "        auprc = average_precision_score(labels_onehot_val, probs, average=None)\n",
    "        auprc_dict = {auprc_keys[i]: v for i, v in enumerate(auprc)}\n",
    "\n",
    "        model_details = {}\n",
    "        model_details[\"foundation_model\"] = foundation_model\n",
    "        model_details[\"aggregator\"] = aggregator\n",
    "        model_details[\"classifier\"] = \"MLP\"\n",
    "        model_details[\"fold\"] = i\n",
    "        model_details = model_details | auroc_dict | auprc_dict\n",
    "        details_df = pd.Series(model_details)\n",
    "        results = pd.concat(\n",
    "            [results, details_df.to_frame().T], ignore_index=True\n",
    "        )\n",
    "\n",
    "    evaluator.finalize(class_freqs)\n",
    "    evaluator.save_figs(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\n",
    "    \"outputs/experiments_by_fold.csv\", sep=\"|\", mode=\"a\", header=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
