{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels data with folds\n",
    "import numpy as np\n",
    "\n",
    "from utils.load_data import load_data\n",
    "from data_models.Label import Label\n",
    "\n",
    "label_path = os.path.join(DATA_DIR, \"labels/labels.csv\")\n",
    "fold_path = os.path.join(DATA_DIR, \"folds.json\")\n",
    "\n",
    "df = load_data(label_path=label_path, fold_path=fold_path)\n",
    "df = df.set_index(\"specimen_id\")\n",
    "labels_onehot = df[Label._member_names_].to_dict(orient=\"split\", index=True)\n",
    "labels_onehot = {\n",
    "    k: np.array(labels_onehot[\"data\"][i])\n",
    "    for i, k in enumerate(labels_onehot[\"index\"])\n",
    "}\n",
    "labels_dict = {row.name: int(row[\"label\"]) for _, row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the absolute path for each slide's set of tile embeddings\n",
    "# tile_embed_dir = \"/opt/gpudata/skin-cancer/outputs/prism/tile_embeddings\"\n",
    "# fnames = os.listdir(tile_embed_dir)\n",
    "# tile_embed_paths = [\n",
    "#     os.path.join(tile_embed_dir, fname)\n",
    "#     for fname in fnames\n",
    "#     if fname.endswith(\".pkl\") and fname[:6] in set(df.index)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.tile_embed_postproc import postproc_pkl\n",
    "\n",
    "# postproc_pkl(tile_embed_dir, tile_embed_dir + \"_sorted\", 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_embed_dir = (\n",
    "    \"/opt/gpudata/skin-cancer/outputs/prism/tile_embeddings_sorted\"\n",
    ")\n",
    "fnames = os.listdir(tile_embed_dir)\n",
    "tile_embed_paths = [\n",
    "    os.path.join(tile_embed_dir, fname)\n",
    "    for fname in fnames\n",
    "    if fname.endswith(\".pkl\") and fname[:6] in set(df.index)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open(tile_embed_paths[0], \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of specimens within each fold\n",
    "specimens_by_fold = df.groupby(\"fold\").groups\n",
    "specimens_by_fold = [list(specs) for specs in specimens_by_fold.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map specimens to slides\n",
    "slides_by_specimen = {spec: [] for spec in list(df.index)}\n",
    "for slide in tile_embed_paths:\n",
    "    slide_name = os.path.basename(slide)[:-4]\n",
    "    spec = slide_name[:6]\n",
    "    if slides_by_specimen.get(spec) is not None:\n",
    "        slides_by_specimen[spec].append(slide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_models.Label import Label\n",
    "\n",
    "class_freqs = {\n",
    "    label: df[label].value_counts(normalize=True).iloc[1]\n",
    "    for label in Label._member_names_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from data_models.datasets import SlideEncodingDataset, collate_tile_embeds\n",
    "from utils.split import train_val_split_slides, train_val_split_labels\n",
    "\n",
    "\n",
    "def get_loaders(\n",
    "    val_fold: int,\n",
    "    specimens_by_fold: List[List[str]],\n",
    "    slides_by_specimen: Dict[str, List[str]],\n",
    "    labels_by_specimen: Dict[str, int],\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    train, val = train_val_split_slides(\n",
    "        val_fold=val_fold,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "        slides_by_specimen=slides_by_specimen,\n",
    "    )\n",
    "    train_labels, val_labels = train_val_split_labels(\n",
    "        val_fold=val_fold,\n",
    "        labels_by_specimen=labels_by_specimen,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        SlideEncodingDataset(train, train_labels),\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_tile_embeds,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        SlideEncodingDataset(val, val_labels),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_tile_embeds,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from operator import itemgetter\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from models.agg import MILClassifier\n",
    "from models.utils.train import train_epoch, val_epoch\n",
    "from utils.eval import Evaluator, get_spec_level_probs\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 4\n",
    "PATIENCE = 10\n",
    "\n",
    "gated = False\n",
    "heads = 1\n",
    "model_name = \"chkpts/prism-abmil-1_head-fold-{i}.pt\"\n",
    "exp_name = \"prism/abmil/ungated/prism-3_hidden\"\n",
    "\n",
    "evaluator = Evaluator(Label)\n",
    "\n",
    "for i in range(len(specimens_by_fold)):\n",
    "    print(f\"--------------------FOLD    {i + 1}--------------------\")\n",
    "    # get dataloaders and embed dims for loaded embeddings\n",
    "    train_loader, val_loader = get_loaders(\n",
    "        i, specimens_by_fold, slides_by_specimen, labels_dict\n",
    "    )\n",
    "    for sample in train_loader:\n",
    "        embed_dim = sample[\"tile_embeds\"].shape[-1]\n",
    "        break\n",
    "\n",
    "    model = MILClassifier(embed_dim, NUM_LABELS, heads, gated).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_weights = None\n",
    "    best_model_data = {\"ids\": None, \"labels\": None, \"probs\": None}\n",
    "    patience = PATIENCE\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optim, loss_fn, BATCH_SIZE\n",
    "        )\n",
    "        val_loss, labels, probs, ids = val_epoch(model, val_loader, loss_fn)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            best_model_data[\"ids\"] = ids\n",
    "            best_model_data[\"labels\"] = labels\n",
    "            best_model_data[\"probs\"] = probs\n",
    "            patience = PATIENCE\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            spaces = \" \" * (4 - len(str(epoch + 1)))\n",
    "            print(\n",
    "                f\"--------------------EPOCH{spaces}{epoch + 1}--------------------\"\n",
    "            )\n",
    "            print(f\"train loss: {train_loss:0.6f}\")\n",
    "            print(f\"val loss:   {val_loss:0.6f}\")\n",
    "            print()\n",
    "\n",
    "    # save the best model\n",
    "    torch.save(\n",
    "        best_model_weights,\n",
    "        os.path.join(OUTPUT_DIR, model_name.format(i=i)),\n",
    "    )\n",
    "\n",
    "    # extract relevant data from val results for best model\n",
    "    ids, probs = get_spec_level_probs(\n",
    "        best_model_data[\"ids\"], best_model_data[\"probs\"]\n",
    "    )\n",
    "    labels_onehot_val = np.array(itemgetter(*ids)(labels_onehot))\n",
    "\n",
    "    evaluator.fold(probs, labels_onehot_val, i, len(specimens_by_fold))\n",
    "\n",
    "evaluator.finalize(class_freqs)\n",
    "evaluator.save_figs(exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
