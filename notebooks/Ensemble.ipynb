{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.chdir(\"..\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the labels data with folds\n",
    "import numpy as np\n",
    "\n",
    "from utils.load_data import load_data\n",
    "from data_models.Label import Label\n",
    "\n",
    "label_path = os.path.join(DATA_DIR, \"labels/labels.csv\")\n",
    "fold_path = os.path.join(DATA_DIR, \"folds.json\")\n",
    "\n",
    "df = load_data(label_path=label_path, fold_path=fold_path)\n",
    "df = df.set_index(\"specimen_id\")\n",
    "labels_onehot = df[Label._member_names_].to_dict(orient=\"split\", index=True)\n",
    "labels_onehot = {\n",
    "    k: np.array(labels_onehot[\"data\"][i])\n",
    "    for i, k in enumerate(labels_onehot[\"index\"])\n",
    "}\n",
    "labels_dict = {row.name: int(row[\"label\"]) for _, row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the tile embedding paths for each tile encoder model\n",
    "def get_tile_embed_paths(tile_embed_dir):\n",
    "    fnames = os.listdir(tile_embed_dir)\n",
    "    fnames.sort()\n",
    "    tile_embed_paths = {\n",
    "        fname[:-4]: os.path.join(tile_embed_dir, fname)\n",
    "        for fname in fnames\n",
    "        if fname.endswith(\".pkl\") and fname[:6] in set(df.index)\n",
    "    }\n",
    "\n",
    "    # return as dict to act as an ordered set\n",
    "    return tile_embed_paths\n",
    "\n",
    "\n",
    "tile_encoders = [\"uni\", \"gigapath\"]\n",
    "tile_embed_path_base = OUTPUT_DIR + \"/{model}/tile_embeddings_sorted\"\n",
    "tile_embed_paths = {\n",
    "    model: get_tile_embed_paths(tile_embed_path_base.format(model=model))\n",
    "    for model in tile_encoders\n",
    "}\n",
    "slide_embeds_path = os.path.join(\n",
    "    OUTPUT_DIR, \"prism/slide_embeddings/prism_slide_embeds_perceiver.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of specimens within each fold\n",
    "specimens_by_fold = df.groupby(\"fold\").groups\n",
    "specimens_by_fold = [list(specs) for specs in specimens_by_fold.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map specimens to slides\n",
    "slides_by_specimen = {spec: [] for spec in list(df.index)}\n",
    "for slide_name in tile_embed_paths[\"uni\"]:\n",
    "    spec = slide_name[:6]\n",
    "    if slides_by_specimen.get(spec) is not None:\n",
    "        slides_by_specimen[spec].append(slide_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_models.Label import Label\n",
    "\n",
    "class_freqs = {\n",
    "    label: df[label].value_counts(normalize=True).iloc[1]\n",
    "    for label in Label._member_names_\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from operator import itemgetter\n",
    "\n",
    "from data_models.datasets import (\n",
    "    SlideEncodingDataset,\n",
    "    EnsembleDataset,\n",
    "    SlideClassificationDataset,\n",
    ")\n",
    "from utils.split import train_val_split_slides, train_val_split_labels\n",
    "\n",
    "\n",
    "def get_datasets(\n",
    "    val_fold: int,\n",
    "    specimens_by_fold: List[List[str]],\n",
    "    slides_by_specimen: Dict[str, List[str]],\n",
    "    labels_by_specimen: Dict[str, int],\n",
    "    tile_embed_paths: Dict[str, Dict[str, str]],\n",
    "    slide_embeds_path: str,\n",
    ") -> Tuple[EnsembleDataset, EnsembleDataset]:\n",
    "    # get the train and val splits\n",
    "    train, val = train_val_split_slides(\n",
    "        val_fold=val_fold,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "        slides_by_specimen=slides_by_specimen,\n",
    "    )\n",
    "    train_labels, val_labels = train_val_split_labels(\n",
    "        val_fold=val_fold,\n",
    "        labels_by_specimen=labels_by_specimen,\n",
    "        specimens_by_fold=specimens_by_fold,\n",
    "    )\n",
    "\n",
    "    # create the datasets\n",
    "    train_slide_encoder_datasets = {\n",
    "        model: SlideEncodingDataset(\n",
    "            list(itemgetter(*train)(tile_embed_paths[model])), train_labels\n",
    "        )\n",
    "        for model in tile_embed_paths\n",
    "    }\n",
    "    val_slide_encoder_datasets = {\n",
    "        model: SlideEncodingDataset(\n",
    "            itemgetter(*val)(tile_embed_paths[model]), val_labels\n",
    "        )\n",
    "        for model in tile_embed_paths\n",
    "    }\n",
    "    train_slide_classifier_dataset = SlideClassificationDataset(\n",
    "        slide_embeds_path, train, train_labels\n",
    "    )\n",
    "    val_slide_classifier_dataset = SlideClassificationDataset(\n",
    "        slide_embeds_path, val, val_labels\n",
    "    )\n",
    "\n",
    "    # create the ensemble datasets\n",
    "    train_set = EnsembleDataset(\n",
    "        train_slide_encoder_datasets\n",
    "        | {\"prism\": train_slide_classifier_dataset}\n",
    "    )\n",
    "    val_set = EnsembleDataset(\n",
    "        val_slide_encoder_datasets | {\"prism\": val_slide_classifier_dataset}\n",
    "    )\n",
    "\n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataset: Dataset,\n",
    "    optimizer: torch.optim,\n",
    "    loss_fn: nn.Module,\n",
    "    grad_accum_steps: int,\n",
    "    device: Optional[torch.device] = None,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Trains an epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to train\n",
    "\n",
    "    dataset : Dataset\n",
    "        The dataset for the training data\n",
    "\n",
    "    optimizer : torch.optim\n",
    "        The optimizer\n",
    "\n",
    "    loss_fn : nn.Module\n",
    "        The loss function\n",
    "\n",
    "    grad_accum_steps : int\n",
    "        The number of batches/samples to accumulate gradients for before\n",
    "        updating the model\n",
    "\n",
    "    device : Optional[torch.device]\n",
    "        The device to send the model and data to. If not provided, the model\n",
    "        and data will not be moved to a device\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average training loss for the epoch\n",
    "    \"\"\"\n",
    "    agg_loss = 0.0\n",
    "    model = model.to(device) if device else model\n",
    "    model.train()\n",
    "    order = np.random.permutation(len(dataset))\n",
    "    for n, i in enumerate(order):\n",
    "        sample = dataset[i]\n",
    "        x = {\n",
    "            \"uni\": sample[\"uni\"][\"tile_embeds\"].unsqueeze(0),\n",
    "            \"gigapath\": sample[\"gigapath\"][\"tile_embeds\"].unsqueeze(0),\n",
    "            \"prism\": sample[\"prism\"][\"slide_embed\"].unsqueeze(0),\n",
    "        }\n",
    "        coords = {\n",
    "            \"uni\": sample[\"uni\"][\"pos\"].unsqueeze(0),\n",
    "            \"gigapath\": sample[\"gigapath\"][\"pos\"].unsqueeze(0),\n",
    "        }\n",
    "        label = torch.tensor([sample[\"uni\"][\"label\"]])\n",
    "\n",
    "        for key in x:\n",
    "            x[key] = x[key].to(device)\n",
    "            if key in coords:\n",
    "                coords[key] = coords[key].to(device)\n",
    "\n",
    "        logits = model(x, coords)\n",
    "        loss = loss_fn(logits, label.to(logits.device))\n",
    "        loss.backward()\n",
    "        agg_loss += loss.item()\n",
    "\n",
    "        # accumulate grad until grad_accum_steps is reached\n",
    "        if (n + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    # ensure no remaining accumulated grad\n",
    "    if (n + 1) % grad_accum_steps != 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return agg_loss / (n + 1)\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model: nn.Module,\n",
    "    dataset: Dataset,\n",
    "    device: torch.device,\n",
    "    loss_fn: Optional[nn.Module] = None,\n",
    ") -> Tuple[float, torch.Tensor, torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Validates an epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        The model to validate\n",
    "\n",
    "    dataset : Dataset\n",
    "        The dataset for the validation data\n",
    "\n",
    "    device : torch.device\n",
    "        The device to send the model and data to\n",
    "\n",
    "    loss_fn : nn.Module, optional\n",
    "        The loss function; if not provided no loss will be calculated\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average training loss for the epoch; if loss_fn is not provided\n",
    "        this will be 0\n",
    "\n",
    "    torch.Tensor\n",
    "        The labels for the validation data\n",
    "\n",
    "    torch.Tensor\n",
    "        The model outputs for the validation data (softmaxed logits)\n",
    "\n",
    "    List[str]\n",
    "        The IDs for the validation data\n",
    "    \"\"\"\n",
    "    agg_loss = 0.0\n",
    "    outputs = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(dataset):\n",
    "            x = {\n",
    "                \"uni\": sample[\"uni\"][\"tile_embeds\"].unsqueeze(0),\n",
    "                \"gigapath\": sample[\"gigapath\"][\"tile_embeds\"].unsqueeze(0),\n",
    "                \"prism\": sample[\"prism\"][\"slide_embed\"].unsqueeze(0),\n",
    "            }\n",
    "            coords = {\n",
    "                \"uni\": sample[\"uni\"][\"pos\"].unsqueeze(0),\n",
    "                \"gigapath\": sample[\"gigapath\"][\"pos\"].unsqueeze(0),\n",
    "            }\n",
    "            label = torch.tensor([sample[\"uni\"][\"label\"]])\n",
    "\n",
    "            for key in x:\n",
    "                x[key] = x[key].to(device)\n",
    "                if key in coords:\n",
    "                    coords[key] = coords[key].to(device)\n",
    "\n",
    "            logits = model(x, coords)\n",
    "            if loss_fn is not None:\n",
    "                loss = loss_fn(logits, label.to(device))\n",
    "                agg_loss += loss.item()\n",
    "\n",
    "            outputs.append(torch.softmax(logits.detach().cpu(), dim=-1))\n",
    "            labels.append(label)\n",
    "            ids.append(sample[\"uni\"][\"id\"])\n",
    "    outputs = torch.cat(outputs)\n",
    "    labels = torch.cat(labels)\n",
    "    return agg_loss / (i + 1), labels, outputs, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from models.ensemble import EnsembleClassifier\n",
    "from utils.eval import Evaluator\n",
    "\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 16\n",
    "NUM_LABELS = 4\n",
    "PATIENCE = 10\n",
    "\n",
    "auroc_keys = [k + \"_auroc\" for k in [\"benign\", \"bowens\", \"bcc\", \"scc\"]]\n",
    "auprc_keys = [k + \"_auprc\" for k in [\"benign\", \"bowens\", \"bcc\", \"scc\"]]\n",
    "results = pd.DataFrame(\n",
    "    columns=[\"foundation_model\", \"aggregator\", \"classifier\", \"fold\"]\n",
    "    + auroc_keys\n",
    "    + auprc_keys\n",
    ")\n",
    "\n",
    "# set experiment-specific variables\n",
    "foundation_model = \"ensemble\"\n",
    "gated = False\n",
    "heads = 1\n",
    "aggregator = \"abmil\"\n",
    "save_directory = \"ungated\"\n",
    "model_name = \"chkpts/{foundation_model}-{aggregator}-{heads}_heads-fold-{i}.pt\"\n",
    "exp_name = f\"{foundation_model}/abmil/{save_directory}/{foundation_model}-3_hidden-{heads}_heads\"\n",
    "\n",
    "evaluator = Evaluator(Label)\n",
    "\n",
    "# for each fold, train and validate while saving the best models\n",
    "# then evaluate the final outputs by generating curves using Evaluator\n",
    "for i in range(len(specimens_by_fold)):\n",
    "    print(f\"--------------------FOLD    {i + 1}--------------------\")\n",
    "    # get dataloaders and embed dims for loaded embeddings\n",
    "    train_set, val_set = get_datasets(\n",
    "        i,\n",
    "        specimens_by_fold,\n",
    "        slides_by_specimen,\n",
    "        labels_dict,\n",
    "        tile_embed_paths,\n",
    "        slide_embeds_path,\n",
    "    )\n",
    "\n",
    "    model = EnsembleClassifier().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_weights = None\n",
    "    best_model_data = {\"ids\": None, \"labels\": None, \"probs\": None}\n",
    "    patience = PATIENCE\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_set,\n",
    "            optim,\n",
    "            loss_fn,\n",
    "            BATCH_SIZE,\n",
    "            device,\n",
    "        )\n",
    "        val_loss, labels, probs, ids = val_epoch(\n",
    "            model,\n",
    "            val_set,\n",
    "            device,\n",
    "            loss_fn,\n",
    "        )\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            best_model_data[\"ids\"] = ids\n",
    "            best_model_data[\"labels\"] = labels\n",
    "            best_model_data[\"probs\"] = probs\n",
    "            patience = PATIENCE\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience == 0:\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            spaces = \" \" * (4 - len(str(epoch + 1)))\n",
    "            print(\n",
    "                f\"--------------------EPOCH{spaces}{epoch + 1}--------------------\"\n",
    "            )\n",
    "            print(f\"train loss: {train_loss:0.6f}\")\n",
    "            print(f\"val loss:   {val_loss:0.6f}\")\n",
    "            print()\n",
    "\n",
    "    # save the best model\n",
    "    torch.save(\n",
    "        best_model_weights,\n",
    "        os.path.join(\n",
    "            OUTPUT_DIR,\n",
    "            model_name.format(\n",
    "                foundation_model=foundation_model,\n",
    "                aggregator=aggregator,\n",
    "                heads=heads,\n",
    "                i=i,\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # extract relevant data from val results for best model\n",
    "    ids, probs = Evaluator.get_spec_level_probs(\n",
    "        best_model_data[\"ids\"], best_model_data[\"probs\"]\n",
    "    )\n",
    "    labels_onehot_val = np.array(itemgetter(*ids)(labels_onehot))\n",
    "\n",
    "    evaluator.fold(probs, labels_onehot_val, i, len(specimens_by_fold))\n",
    "    auroc = roc_auc_score(\n",
    "        labels_onehot_val, probs, average=None, multi_class=\"ovr\"\n",
    "    )\n",
    "    auroc_dict = {auroc_keys[i]: v for i, v in enumerate(auroc)}\n",
    "\n",
    "    auprc = average_precision_score(labels_onehot_val, probs, average=None)\n",
    "    auprc_dict = {auprc_keys[i]: v for i, v in enumerate(auprc)}\n",
    "\n",
    "    model_details = {}\n",
    "    model_details[\"foundation_model\"] = foundation_model\n",
    "    model_details[\"aggregator\"] = aggregator\n",
    "    model_details[\"classifier\"] = \"MLP\"\n",
    "    model_details[\"fold\"] = i\n",
    "    model_details = model_details | auroc_dict | auprc_dict\n",
    "    details_df = pd.Series(model_details)\n",
    "    results = pd.concat([results, details_df.to_frame().T], ignore_index=True)\n",
    "\n",
    "evaluator.finalize(class_freqs)\n",
    "evaluator.save_figs(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(\n",
    "    \"outputs/experiments_by_fold.csv\", sep=\"|\", mode=\"a\", header=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
